var documenterSearchIndex = {"docs":
[{"location":"guide/sanity_checks/#Sanity-Checks","page":"Sanity Checks","title":"Sanity Checks","text":"We can run several sanity checks to ensure that our Neural ODE is indeed learning the Schwarzschild Metric. In this module, we examine the following sanity checks: \n\nnote: Use Schwarzchild Metric as Base Model (Multiplicative Corrections)\nIf we use the Schwarzschild Metric as the base model for the Neural ODE, there should be nothing for the network to learn. We are considering training data generated by using schwarzschild geodesics (no dissipation, only conservative dynamics). For this particular sanity check, we consider only circular orbits, the simplest kind of orbit. We can frame this in one of two ways. We can have multiplicative corrections as follows:# Base Metric: Schwarzschild Metric\n  g = [\n        -f^(-1)*f_tt_NN_correction 0 0 0;\n        0 f*f_rr_NN_correction 0 0;\n        0 0 0 0;\n        0 0 0 r^(-2)*f_ϕϕ_NN_correction\n    ]\n\nnote: Use Schwarzchild Metric as Base Model (Additive Corrections)\nIn the multiplicative case, the corrections that the neural network should learn is 1. Alternatively, we can have additive corrections as follows:  # Base Metric: Schwarzschild Metric\n  g = [\n        -f^(-1)+f_tt_NN_correction 0 0 0;\n        0 f+f_rr_NN_correction 0 0;\n        0 0 0 0;\n        0 0 0 r^(-2)+f_ϕϕ_NN_correction\n    ]The additive corrections should be 0 if the neural network learns \"nothing\" correctly. Ideally, this sanity check would be the simplest thing for the neural network to do: in the multiplicative correction case, it should learn no more than 1. Likewise in the additive correction case, it should learn simply 0. \n\nFirst of all, when conducting sanity checks, you want to strip the problem of all its complexity. For starters, we make the neural network extremely simple.\n\nNN_Conservative = Chain(\n    Dense(1, 1, tanh), # Input: r only\n    Dense(1, 1, tanh),\n    Dense(1, 3), # Output: Corrections for [g^tt, g^rr, g^ϕϕ]\n)\n\nWe will treat the corrections as multiplicative for this first case, as follows: \n\ng = [\n      -f^(-1)*f_tt_NN_correction 0 0 0;\n      0 f*f_rr_NN_correction 0 0;\n      0 0 0 0;\n      0 0 0 r^(-2)*f_ϕϕ_NN_correction\n    ]\n\nThis Neural Network has only 10 parameters: \n\nChain(\n    layer_1 = Dense(1 => 1, tanh),      # 2 parameters\n    layer_2 = Dense(1 => 1, tanh),      # 2 parameters\n    layer_3 = Dense(1 => 3),            # 6 parameters\n )        # Total: 10 parameters,\n          #        plus 0 states.\n\nWe initialize the NN weights and biases for the hidden layers, except for the final layer: \n\nfor (i, layer) in enumerate(NN_Conservative_params)\n    if ~isempty(layer)\n        if i == length(NN_Conservative_params)  # Final layer\n            layer.weight .= 0\n            layer.bias .= 0  # Force output near 0\n        else  # Hidden layers\n            layer.weight .= 0.1 * randn(rng, eltype(layer.weight), size(layer.weight))\n            layer.bias .= 0.1 * randn(rng, eltype(layer.bias), size(layer.bias))\n        end\n    end\nend\n\nWhen we do this, here's what we find for the NN parameters (conservative NN) after initialization (but before training): \n\nlayer_1 = (weight = [0.23716215388290493;;], bias = [0.12095544082478167])\nlayer_2 = (weight = [0.03983070751935526;;], bias = [-0.16211640815748582])\nlayer_3 = (weight = [0.0; 0.0; 0.0;;], bias = [0.0, 0.0, 0.0]))\n\nWhen you plot the initial solution to the ODEs, we find exact agreement (as expected, since our base model is the Schwarzschild metric!)\n\n(Image: CircularOrbit)\n\nNow when we plot the predicted metrics and orbit after initialization (but before training), we find exact agreement, as expected:\n\n(Image: 3Corrections_PreTraining)\n\nOur loss function is simply a mean-squared error between the predicted and true waveform: \n\nfunction loss(NN_params; saveat=tsteps)\n    tspan = (saveat[1],saveat[end])\n    pred_soln = solve(remake(prob_nn_dual, p = NN_params, tspan=tspan), Tsit5(),\n                            saveat = saveat, dt = dt, adaptive=false, verbose = false, sensealg=BacksolveAdjoint(checkpointing=true))\n    pred_waveform_real, pred_waveform_imag = compute_waveform(dt_data, pred_soln, mass_ratio)\n\n    loss = ( sum(abs2, waveform_real_ecc .- pred_waveform_real))\n    return loss\nend\n\nWhen we run our loss function on just the initial solution, we get 2.25e-7. Now we run the training process, after which we obtain these results: \n\n(Image: 3Correction_PostTraining)\n\nI ran through 4 iterations: optimization_increments = [1, 2, 12, 20]. Over these four iterations, here's how the weights and biases changed over time: \n\nwarning: Iteration #1 Weights & Biases\nlayer_1 = (weight = [0.2371808185363373;], bias = [0.120935736109229])\nlayer_2 = (weight = [0.03982656907286769;], bias = [-0.16213122300485072]) \nlayer_3 = (weight = [-1.821512530359164e-5; -1.0175274439859679e-5; 8.716160136546225e-7;], bias = [9.8766420116128e-6, 5.723394566465591e-6, 3.481497345221852e-6]))\n\nwarning: Iteration #2 Weights & Biases\nlayer_1 = (weight = [0.23718676030213381;], bias = [0.12092385329517617])\nlayer_2 = (weight = [0.03983349725191167;], bias = [-0.1621299634300346])\nlayer_3 = (weight = [-2.487679834028046e-5; -2.887058705740126e-5; 1.0738885848191339e-6;], bias = [-4.71782709892314e-6, 7.837322991384513e-6, 2.537287450937874e-6]))\n\nwarning: Iteration #12 Weights & Biases\nlayer_1 = (weight = [0.2371894925389875;], bias = [0.12093772915528389])\nlayer_2 = (weight = [0.03983157793305357;], bias = [-0.16213630211015895])\nlayer_3 = (weight = [-1.6052012110502056e-5; -3.96908865801902e-5; -9.619462849749647e-6;], bias = [-2.1939207002792404e-5, 1.500639545614688e-6, -2.168304595713335e-5]))\n\nwarning: Iteration #20 Weights & Biases\nlayer_1 = (weight = [0.23718368381374658;], bias = [0.12094224166402442])\nlayer_2 = (weight = [0.03983587381877536;], bias = [-0.16213807102352437])\nlayer_3 = (weight = [-9.10753409970219e-6; -4.110947128717602e-5; -1.749976849031467e-5;], bias = [-1.7950002563821174e-5, 1.4704911668732934e-5, -1.9016787781020368e-5]))\n\nHere are the loss function and state variables post-training for the sanity check: \n\n(Image: State_Variables_PostTraining)\n\n(Image: LossFunction_PostTraining)\n\nAfter printing f_tt_pred, f_rr_pred, and f_\\phi\\phi_pred, the corrections to the g^tt g^rr g^phiphi, we find: \n\njulia> f_tt_pred\n400-element Vector{Any}:\n 0.9999831633432387\n 0.999983162958138\n 0.9999831626034161\n 0.9999831622767038\n 0.9999831619758125\n 0.9999831616987213\n 0.999983161443564\n ⋮\n 0.9999831584900172\n 0.9999831584900172\n 0.9999831584900172\n 0.9999831584900172\n 0.9999831584900172\n 0.9999831584900172\n\nf_rr_pred\n400-element Vector{Any}:\n 1.0000197298726115\n 1.0000197281342853\n 1.0000197265330875\n 1.0000197250583238\n 1.000019723700115\n 1.0000197224493381\n 1.0000197212975706\n ⋮\n 1.000019707965401\n 1.000019707965401\n 1.000019707965401\n 1.000019707965401\n 1.000019707965401\n 1.000019707965401\n\nf_pp_pred\n400-element Vector{Any}:\n 0.9999831223324658\n 0.99998312159251\n 0.9999831209109258\n 0.9999831202831609\n 0.9999831197050103\n 0.9999831191725903\n 0.9999831186823158\n ⋮\n 0.9999831130071919\n 0.9999831130071919\n 0.9999831130071919\n 0.9999831130071919\n 0.9999831130071919\n 0.9999831130071919\n\nThus, the sanity check has been passed for multiplicative corrections for a circular orbit! Now we consider additive corrections!\n\n# Base Metric: Schwarzschild Metric\ng = [\n        -f^(-1)+f_tt_NN_correction 0 0 0;\n        0 f+f_rr_NN_correction 0 0;\n        0 0 0 0;\n        0 0 0 r^(-2)+f_ϕϕ_NN_correction\n    ]\n\nWhen we run the sanity check for additive corrections with three corrections for g^tt g^rr g^phiphi, here are the results: \n\n(Image: AdditiveCorrections_3Terms)\n\nNow if we return only two corrections, for g^rr and g^phiphi and lock in g^tt, we obtain","category":"section"},{"location":"guide/inverse_problem/#Fundamentals-of-Neural-Networks","page":"Fundamentals of Neural Networks","title":"Fundamentals of Neural Networks","text":"We will be using the Lux.jl library to create neural networks in Julia. Here's what we'll need to import: \n\nusing Lux # to create our neural network\nusing Random # to initialize our network\nusing Zygote # automatic differentiation engine in Julia (used for backpropagation)\nusing Optimisers # provides gradient descent algorithm\nusing Plots # to create plots\nusing Distributions # to create a toy dataset\nusing Statistics # provides the mean function\n\nLet's now define the hyperparameters of the network:\n\nN_SAMPLES = 200 # number of data samples for our toy dataset\nLAYERS = [1, 10, 10, 10, 1] # 1D input --> 1D output; 3 hidden layers with 10 dimensions\nLEARNING_RATE = 0.1\nN_EPOCHS = 30,000\n\nNow we need a psuedo-random number generator\n\nrng = Xoshiro(42)\n\nNow we can produce our toy dataset:\n\nx_samples = rand( # produce random samples\n                 rng, # provide random # generator\n                 Uniform(0, 2 * pi)) # define what distribution you want to sample from\n                 (1, N_SAMPLES) # array that we want to sample; 1 is the spatial dimension and N_SAMPLES is the batch dimension\n                )\n\nNow let's create some noise for our data: \n\ny_noise = rand(\n              rng, # random number generator\n              Normal(0, 0.3) # Create noise from normal distribution with center at 0.0 and standard deviation of 0.3\n              (1, N_SAMPLES) # Draw n by n sample values because we want to corrupt each sample value individually\n              )\n\nNow let's create a noisy sine function from these parts: \n\ny_samples = sin.(x_samples) .+ y_noise\n\nNow we plot this by doing scatter(x_samples[:], y_samples[:], label = \"data\"). \n\n(Image: ScatterPlot)\n\nNow let's define the architechture of our neural network. Since we'll create a simple feedforward neural network, we'll express oits architechture in the form of a chain of functions: \n\nmodel = Chain( # NN = nested chain of functions\n             [Dense(fan_in => fan_out, Lux.sigmoid) for (fan_in, fan_out) in zip(LAYERS[1:end-2], LAYERS[2:end-1])] ... # sigmoid activation function for the transition between each layer\n             Dense(LAYERS[end-1] => LAYERS[end], identity) # last layer has no activation function, only first three layers has sigmoid activation\n            )\n\nIn total, our network has 251 parameters. What does that mean? We will now initialize the parameters for the NN, as well as the layer states. \n\nparameters, layer_states = Lux.setup(rng, model)\n\nNow let's make a prediction using the initial 250 parameters of the neural network.  \n\ny_initial_prediction, layer_states = model(x_samples, parameters, layer_states) # carry the parameters and layer states of the NN through\n\nNow let's see what the initial prediction of the NN is: \n\nscatter!(x_sample[:], y_initial_prediction[:], label = \"initial prediction\")\n\n(Image: InitialPrediction)\n\nWe will now define a loss function\n\nfunction loss_fn(p, ls)\n  y_prediction, new_ls = model(x_samples, p, ls)\n  loss = 0.5 * mean((y_prediction .- y_samples).^2)\n  return loss, new_ls\nend\n\nCreate an optimizer which uses gradient descent (or you can use ADAM or BFGS if you'd like):\n\nopt = Descent(LEARNING_RATE)\nopt_state = Optimisers.setup(opt, parameters) # optimize / minimize the loss function over the parameters of the NN\n\nNow, we define a training loop\n\nloss_history = []\nfor epoch in 1:N_EPOCHS\n  (loss, layer_states,), back = pullback(loss_fn, parameters, layer_states) # we use (loss, layer_states) to get the output of the loss function, which returns a tuple\n  grad, _ = back((1.0, nothing))\n\n  opt_state, parameters = Optimisers.update(opt_state, parameters, grad)\n  push!(loss_history, loss)\n\n  if epoch % 100 == 0\n    println(\"Epoch: $Epoch, Loss: $Loss\")\n  end\nend\n\nNow we can plot the loss function by plot(loss_history, yscale =:log10). \n\n(Image: LossFunction)\n\nWe now see that the final prediction is much better than the initial one! \n\ny_final_prediction, layer_states = model(x_samples, parameters, layer_state)\nscatter(x_samples[:], y_samples[:], label = \"data\")\nscatter!(x_samples[:], y_final_prediction[:], label = \"final prediction\")\n\n(Image: FinalFit)","category":"section"},{"location":"guide/waveforms/#Quadrupole-Approximation:-Orbits-to-Gravitational-Wave","page":"Quadrupole Approximation: Orbits to Gravitational Wave","title":"Quadrupole Approximation: Orbits to Gravitational Wave","text":"How to install, load data, and run a tiny example…","category":"section"},{"location":"guide/GPU/#Running-Black-Holes-on-GPUs","page":"Running Black Holes on GPUs","title":"Running Black Holes on GPUs","text":"GPU: Graphics Processing Unit CPU: Central Processing Unit\n\nIn a login node in Oscar: To run your scripts, you can request a batch job via a Scheduler like slurm. Slurm is a set of instructions (i.e., a script). It determines when you get the job amongst several requests. Then your job starts running on the compute nodes. \n\nHow do you run your program on Oscar? \n\nInteractive Jobs: Program runs in dedicated terminal window\nBatch jobs: Run program in background\n\nSlurm needs to know the following before it can schedule your job, whether it's interactive or batch: \n\nHow many nodes and cores do you need?\nHow much memory do you need?\nDo you require a GPU?\nHow much time does your job need?\n\nTo run interactive jobs:  interact -q gpu -g 1 -n 1 -m 32g -t 1:00:00 Where -g is the number of GPUs, -n is the number of cores, -m is the memory expected to be used by the job, and -t is the time. If you leave all the parameters blank, you'll get a default of 4 GB of memory and 30 minutes of time on the GPU. \n\nTo submit your batch file, do sbatch filename.sh. To check on your jobs, use myjobinfo. Or, alternatively, myjobinfo -j <JobID>","category":"section"},{"location":"guide/newtonian-metric/#Newtonian-Metric","page":"Physics of Newtonian Metric","title":"Newtonian Metric","text":"The goal of this module is to explain the Newtonian Metric which is used as the base model for the Neural Network for Conservative Dynamics. This module should answer the following questions: \n\nWhat is the Newtonian Metric?\nHow does it give rise to Newton's Equations of Motion?\nIn what limit does the Schwarzschild Metric reduce to the Newtonian Metric?\n\nAccording to Einstein's Theory of General Relativity, Flat Space is given by the Minkowski Metric: \n\ng=beginpmatrix \n    -1  0  0  0  \n    0  1  0  0  \n    0  0  1  0  \n    0  0  0  1\n    endpmatrix rightarrow ds^2 = -dt^2 + dx^2 + dy^2 + dz^2\n\nThe Newtonian Metric is given by\n\ng=beginpmatrix      -left(1-frac2GMr right)  0  0  0       0  left(1+frac2GMr right)  0  0       0  0  left(1+frac2GMr right)  0       0  0  0  left(1+frac2GMr right)     endpmatrix\n\nds^2 = -left(1-frac2GMr right)dt^2 + left(1+frac2GMr right)(dx^2 + dy^2 + dz^2)\n\nThis metric is valid in the limit that fracGMr1. The Newtonian Metric represents a small deviation from flat spacetime due to gravity. Note that we will often use the variable phi = -GMr in this module to represent the Newtonian gravitational potential. Please do not confuse this with any sort of angular variable. It is simply the Newtonian gravitational potential, and I'm using it because that is the convention in the book \"A First Course in General Relativity\" by Bernard Schutz. \n\n(Image: SpacetimeMetrics)\n\nObserve that for rM, we see that the Newtonian Metric reduces back to the Minkowski Metric. We will now prove that the Newtonian Metric is so-called because the equations of motion for this metric is, indeed, F=ma. We begin by writing down the geodesic equation, which is the equivalent of Newton's law of Inertia in relativity: An object with no external forces acting on it will move in a straight line. The geodesic equation states that the covariant derivative of the velocity vector is 0:   nabla_UU = 0 Since p^mu = mU^mu, this is equivalent to nabla_p p =0. By expanding the covariant derivative, we obtain: \n\nfracdp^mudtau + Gamma^mu_alpha betap^alphap^beta=0\n\nAlthough the derivation of the covariant derivative is a bit involved, we can think of it simply: The covariant derivative is simply a derivative on a curved manifold. If we are in flat space, then the christoffel symbols Gamma sim 0, so that the covariant derivative and regular derivative are the same: nabla_p p = dpdx^p. However, the christoffel symbol encodes the degree to which a manifold is curved, and thus adds a correction factor to the derivative. \n\nmu\n\nis an index which varies from mu = 0 1 2 3, where mu=0 is associated with the time-component of a four-vector and mu = 123 are associated with the spatial components. Thus, we will consider two cases: mu = 0 (the time component) and mu =123 (the spatial components). We begin with mu=0: \n\nfracdp^0dtau+Gamma^0_alphabetap^alphap^beta=0\n\nWe will now enforce the limit that vc. We are working in natural units where c=1. In the limit that vc, we have p^ip^0. Why? Consider the following: p^mu=m U^mu=mgamma(1 vecv)=(mgamma mvecvgamma). Thus:\n\nfracp^ip^0=fracmvecvgammamgamma=v^i  1 rightarrow p^i  p^0\n\nThe p^0 component of momentum is the total energy. Why? To see this, we recall that the invariant magnitude of the four-momentum is \n\np^mu p_mu = -m^2 to -(p^0)^2 + vecp^2 = -m^2 to (p^0)^2 = m^2 + vecp^2\n\nComparing this to E^2=(mc^2)^2 + (pc)^2 to E^2 = m^2 + vecp^2 (since c=1), we immediately make the connection that E = p^0.\n\nnote: Interpretation of $p^i<<p^0$\np^i=mv^i gammaand p^0=mgamma. The statement that p^ip^0 simply means that most of the particle's energy is stored in its rest mass energy, not in its motion. In other words, the kinetic energy of the particle is much less than its rest energy. \n\nBecause p^i  p^0, we can neglect any terms involving the spatial components of the momentum four-vector. Thus, the geodesic equation reduces from \n\nfracdp^0dtau+Gamma^0_alphabetap^alphap^beta=0\n\nTo alpha = beta =0: \n\nfracdp^0dtau+Gamma^0_00(p^0)^2 =0\n\nnote: Calculation of Christoffel Symbol $\\Gamma^0_{00}$\nWe expand the Christoffel Symbol in terms of the metric asGamma^0_00=frac12g^0alpha(g_alpha 0 + g_alpha 0 - g_00 alpha)These terms will survive only if alpha = 0, which givesGamma^0_00 = frac12g^00(g_000+g_000-g_000)=frac12g^00g_000The Newtonian Metric g_munu is given by g_munu=beginpmatrix -(1+2phi)  0  0  0\n0  (1-2phi)  0  0 \n0  0  (1-2phi)  0 \n0  0  0  (1-2phi) \nendpmatrixThus, using g^00=-(1+2phi)^-1 and g_000=-2dotphi, the Christoffel symbol reduces to Gamma^0_00 = frac12g^00g_000 = frac12cdot -(1+2phi)^-1 cdot -2dotphi = fracdotphi1+2phiWe're almost there! If we now write this as dotphi(1+2phi)^-1approx dotphi(1-2phi)sim dotphi-2phidotphi, we can neglect the second term because it's of order O(phi^2), so that ultimately, Gamma^0_00sim dotphi. \n\nSubstituting in the christoffel symbol Gamma^0_00=dotphi and taking (p^0)^2 = m^2 + vecp^2 sim m^2 (when p^ip^0), we obtain \n\nfracdp^0dtau+dotphi (m^2) = 0 rightarrow boxedfracdp^0dtau=-dotphim^2\n\nThis is an expression of conservation of energy. In particular, the energy of the system p^0 is conserved if the gravitational potential phi = -GMr is constant in time. We now consider the spatial components of the geodesic equation: \n\nfracdp^idtau+Gamma^i_alphabetap^alphabeta=0\n\nBy writing fracdp^idtau=fracdp^idx^alphafracdx^alphadtau=p^i_alpha p^alpha, we have\n\np^i_alphap^alpha + Gamma^i_alphabetap^alphap^beta=0\n\nWe again enforce the low-speed approximation that vc, which means p^ip^0, so that we can once again discard any terms involving p^i:\n\np^i_0p^0 + Gamma^i_00p^0 p^0 =0 rightarrow p^i_0p^0 + Gamma^i_00(p^0)^2=0\n\nOnce again using p^0 sim m, we have \n\nm fracdp^idtau + Gamma^i_00*m^2 = 0\n\nWe thus have \n\nfracdp^idtau = -mGamma^i_00\n\nnote: Calculation of Christoffel Symbol $\\Gamma^i_{00}$\nWe expand the Christoffel Symbol in terms of the metric asGamma^i_00=frac12g^ialpha(g_alpha 00+g_alpha 0 0 - g_00 alpha)These terms are non-zero only in g^ialpha if alpha = i, givingGamma^i_00=frac12g^ii(g_i00+g_i00-g_00i)=-frac12g^iig_00iNow we have g_00=-(1+2phi) rightarrow g_00i=-2phi_i and g^ii=(1-2phi)^-1=-frac12(1-2phi)^-1(-2phi_i). This becomes Gamma^i_00=fracphi_i1-2phi=phi_i(1-2phi)^-1 approx phi_i(1+2phi) sim phi_i+O(phi^2). Neglecting all higher-order terms, we end up with Gamma^i_00sim phi_i.\n\nIf we now substitute the christoffel symbol Gamma^i_00=phi_i, we get \n\nfracdp^idtau=-mGamma^i_00=-mphi_i to fracdp^idtau=-mfracdphidx^i to boxedfracdp^idtau=-mnabla phi","category":"section"},{"location":"guide/higher_modes/#Adding-Higher-Order-Modes","page":"Adding Higher Order Modes","title":"Adding Higher Order Modes","text":"How to install, load data, and run a tiny example…","category":"section"},{"location":"guide/GENERIC/#Theory:-GENERIC-Formalism","page":"Theory: GENERIC Formalism","title":"Theory: GENERIC Formalism","text":"How to install, load data, and run a tiny example…","category":"section"},{"location":"guide/full_inverse_problem/#Dissipative-Case:-Waves-to-Orbits","page":"Dissipative Case: Waves to Orbits","title":"Dissipative Case: Waves to Orbits","text":"How to install, load data, and run a tiny example…","category":"section"},{"location":"guide/schwarzschild-metric/#Schwarzschild-Metric","page":"Physics of Schwarzschild Metric","title":"Schwarzschild Metric","text":"The goal of this module is to generate conservative dynamics training data for the Neural ODE. In particular, this training module will generate the gravitational waveforms associated with two Schwarzschild black holes orbiting each other. These waveforms will subsequently be used as training data for the Neural ODE, which will attempt to reconstruct the orbits. In doing so, the Neural ODE will attempt to learn the Scwhwarzschild Metric from a Newtonian weak-field metric. \n\nIn other words, this module will provide a step-by-step guide to creating a function that has the following input and output: \n\nnote: Generate Schwarzschild Gravitational Waves Function\nThe Generate_Schwarzschild_GWs() function will operate as follows: Goal: Generate training data for conservative dynamics of binary black hole system that will be fed to Neural ODE\nInput: Mass Ratio q=m_2m_1 & Initial Conditions dotx(0)\nOutput: True Orbits (x_1(t) y_1(t)) and (x_2(t) y_2(t)) of a binary black hole system and the associated gravitational waveform h(t). \n\nWe seek to describe the two body problem in general relativity. In particular, we seek the equations of motion for two body dynamics. In the limit that one black hole is much larger than the other, we can describe the smaller black hole as a particle following geodesics of the Schwarzschild metric around the larger black hole. We now proceed to describe how the two body problem can thus be reduced to an effective one-body problem. Plot:\n\n(Image: TestingOrbits)\n\nConsider two black holes of masses m_1 and m_2 orbiting around their common center-of-mass. We have two cases: \n\nm_1m_2\n: The equivalent one-body picture is a particle orbiting a central Schwarzschild black hole of mass M=m_1+m_2\nm_1sim m_2\nThe equivalent one-body picture is a given by the Effective One-Body formalism, wherein the two-body dynamics is mapped to a single-body of reduced mass mu=m_1m_2  (m_1+m_2) orbiting a central mass M = m_1+m_2.\n\nFor reasons of simplicity and application to EMRIs, we will focus on the first option above. Our approach will be as follows: to describe the dynamics of the binary black hole system, we will transform to an effective one-body problem as per the m_1m_2 case above, and then subsequently transform back to the two-body problem. The Hamiltonian describing a particle orbiting a Schwarzschild black hole is \n\nH = fracp^22= frac12p^mu g_mu nu p^nu\n\nThe Schwarzschild Metric g_munu is \n\ng^textSchwarzschild_munu = \nbeginpmatrix\n    -left(1-frac2Mr right)  0  0  0 \n    0  left(1-frac2Mr right)^-1  0  0  \n    0   0  r^2  0  \n    0  0  0  r^2 sin^2 theta\nendpmatrix\n\nThus, the Hamiltonian for the Schwarzschild Metric is \n\nH_Schwarzschild=- left(1-frac2Mrright)^-1fracp_t^22 + left(1-frac2Mrright) fracp_r^22 + fracp_phi^22r^2\n\nH=frac12left(p_t p_r p_theta p_phi right)^T beginpmatrix\n    -left(1-frac2Mrright)^-1  0  0  0\n    0  left(1-frac2Mrright)  0  0  \n    0  0  0  0 \n    0  0  0  r^-2\n    endpmatrix\t \n    left(p_t p_r p_theta p_phi right)\n\nWe formulate this in Julia as follows: \n\nfunction H(state_vec)\n        t, r, θ, φ, p_t, p_r, p_θ, p_ϕ = state_vec\n\n        f = (1 - (2/r))\n        p = [p_t, p_r, p_θ, p_ϕ]\n        g = [\n                -f^(-1) 0 0 0;\n                0 f 0 0;\n                0 0 0 0;\n                0 0 0 r^(-2)\n            ]\n\n        H_schwarzschild = (1/2) * p' * g * p\n\n        return H_schwarzschild # Returns equations of motion in _proper_ time\n    end\n\nWe can now use Hamilton's Equations of Motion to obtain the orbit: \n\ndotq=fracpartial Hpartial p -dotp=fracpartial Hpartial q\n\nThus, for a particle on an equatorial orbit around a Schwarzschild black hole, we obtain the following geodesic equations of motion: \n\nfracdtdtau = left(1-frac2Mrright)^-1E fracdrdtau = left(1-frac2Mrright)p_r fracdthetadtau =0 fracdphidtau = fracLr^2\n\nSimilarly, the momentum vector p^mu evolves as\n\nfracdp_tdtau = dot E = 0\n\nfracdp_rdtau =  -frac12leftleft(1-frac2Mrright)^-2left( frac2Mr^2right) (p_t)^2 + frac2Mr^2(p_r)^2-2r^-3 (p_phi)^2right\n\ndot p_theta = 0 dot p_phi = dot L = 0\n\nWe have thus obtained all the equations of motion for a test particle orbiting a Schwarzschild Black Hole. In Julia, this is simply: \n\ngrad_H = ForwardDiff.gradient(H, x)\n\nL = [\n        zeros(4, 4) I(4);\n        -I(4) zeros(4, 4)\n    ]\n\nConservative = L * grad_H\n\nIn General Relativity, there are two kinds of time: the proper time and coordinate time. There remains a subtle step: Our trajectories are parametrized in terms of proper time tau. To convert back to coordinate time, which is the time measured by a detector here on Earth, we must multiply the equations of motion by a conversion factor. For instance:\n\nfracdrdt=fracdrdtaufracdtaudt=fracdrdtauleft(fracdtdtauright)^-1=fracdrdtauleft(fracpartial Hpartial p_tright)^-1\n\nIn Julia, we formulate this as: \n\ndx_dτ = Conservative\n\ndH_dpₜ = grad_H[5]\ndτ_dt = (dH_dpₜ)^(-1)\n\nfor i = 1:8\n    dx[i] = dx_dτ[i] * dτ_dt\nend\n\nNow that we have all the equations of motion, we return them as an output of the GENERIC() function:\n\nfunction GENERIC(du, u, model_params, t;\n                              NN=nothing, NN_params=nothing)\n    x = u[1:8]\n    q = mass_ratio\n    M = 1.0\n\n    function H(state_vec)\n        # Returns Schwarzschild Hamiltonian\n        H_schwarzschild = (1/2) * p' * g * p\n        return H_schwarzschild\n    end\n\n    # ... [Use Hamilton Equations to get equations of motion]\n\n    # Return equations of motion\n    return [du[1], du[2], du[3], du[4], du[5], du[6], du[7], du[8]]\nend\n\nNow that we have the GENERIC() function, we extract the ODEs from it: \n\nfunction ODE_model_dual(du, u, p, t)\n    du = GENERIC(du, u, model_params, t)\n    return du\nend\n\nNow we define the ODE problem, solve it, and convert it to a waveform!\n\nprob = ODEProblem(ODE_model_dual, u0, tspan)\nsoln = Array(solve(prob, Tsit5(), saveat = tsteps, dt = dt, adaptive=false, verbose=false))\ngw = compute_waveform(soln)\n\nHow do we actually visualize the orbits? We can plot the effective one-body orbit via \n\norbit = soln2orbit(soln)\nplot(orbit[1,:], orbit[2,:], aspect_ratio=:equal)\n\nsoln2orbit() is simply a function which extracts the (rphi) components of the state vector solution and converts them to (xy): \n\nfunction soln2orbit(soln, model_params=nothing)\n    #=\n        Performs change of variables:\n        (r(t),ϕ(t)) ↦ (x(t),y(t))\n    =#\n\n    r = soln[2,:] # radial coordinate solutions\n    ϕ = soln[4,:] # planar angle solutions\n\n    x = r .* cos.(ϕ)\n    y = r .* sin.(ϕ)\n\n    orbit = [x'; y']\n    return orbit\nend\n\nTo visualize the actual, original 2-body problem of the two binary black holes, we use the one2two() function, which implements the following transformation to go from the state vector of the effective one body problem r(t) to the state vectors of the two body problem:  M = m_1 + m_2 r_1 = fracm_2M * textpath r_2 = -fracm_1M * textpath\n\nWhy is it that r_1 propto m_2 and vice-versa? This can be easily understood: In the limiting case that m_2m_1 q=1 Msim m_2. Thus, r_1 sim 1*textpath r_2 sim 0. This is reasonable, as the second mass has negligible motion if its much more massive than the first mass. In Julia, this becomes:\n\nfunction one2two(path, m1, m2)\n    #=\n        We need a very crude 2-body path\n\n        Assume the 1-body motion is a newtonian 2-body position vector r = r1 - r2\n        and use Newtonian formulas to get r1, r2\n        (e.g. Theoretical Mechanics of Particles and Continua 4.3)\n    =#\n\n    M = m1 + m2\n    r1 = m2/M .* path\n    r2 = -m1/M .* path\n\n    return r1, r2\nend\n\nFinally, if we'd like to obtain the gravitational wave from the two-body dynamics instead of the effective one-body dynamics (a good sanity check that they result in the same h(t)), we can employ: \n\nh_plus_true, h_cross_true = h_22_strain_two_body(dt_data, blackHole_r1, mass1, blackHole_r2, mass2)\n\nThat's it! We're done! We've generated the orbits and waveforms associated with the Schwarzschild Metric! (Image: TestingOrbits) (Image: TestingOrbits)","category":"section"},{"location":"api/#API","page":"API","title":"API","text":"```@docs BlackHoles.compute_waveform BlackHoles.GENERIC BlackHoles.file2trajectory less Copy code","category":"section"},{"location":"guide/optimize/#Attempts-to-Optimize-Neural-Network-Fits","page":"Attempts to Optimize Neural Network Fits","title":"Attempts to Optimize Neural Network Fits","text":"In this section, I recount five experiments I ran on the weekend of 11/08 and week of 11/10 to optimize the neural network fits: (1) Tikhonov Regularization, (2) Testing Different Parameters in the Optim.jl library, (3) Variable Transformation to u=1r, (4) Neural Network Size, and (5) Learnable Initial Conditions. \n\nnote: Tikhonov Regularization\nTesting test test\n\nnote: `Optim.jl` Parameters\nThe heart and soul of the optimizer is the BFGS Algorithm. We now provide a brief intro to the BFGS algorithm. The idea is simple: we have some function f(x) which we seek to minimize. To understand BFGS, we turn to its predecessor, Newton's method. Newton's method forms a second order approximation of f(x) at a given point and finds the minimum of this approximation. Specifically, say we'd like to minimize f(x) starting at x_0. If we take a linear Taylor expansion of f(x), we have f(x) approx f(x_0)+f(x_0)(x-x_0). If we seek to minimize f(x), then we seek f(x) = 0. In that case, f(x_0) + f(x_0)x-f(x_0)x_0 = 0. If we now solve for x, we find that x^* = x_0 - fracf(x_0)f(x_0). This method of finding the minimum of a function is known as Newton's method. Newton's method requires both the gradient nabla f = f(x) and the Hessian nabla^2 f = f(x). We can write Newton's method as x^* = x_0 - nabla^2 f(x_0)^-1 nabla f(x_0). But this can be difficult for several reasons: finding the inverse Hessian could be difficult. What if there was a way we could approximate the Hessian (or its inverse)? We now consider the BFGS algorithm. Consider again our goal of minimizing the function f(x). We are iteratively updating the step x_k where k=1 2  T, where T is determined by some convergence criterion. Suppose we use a quadratic approximation to f at each iteration. Denote this approximation at step k as hatf_k(x). Specifically, hatf_k=f(x_k) + nabla f(x_k)^T (x-x_k) + frac12(x-x_k)^T nabla f(x_k)^2 (x-x_k).\n\nnote: Neural Network Size\nBreadth and Depth of Neural Network\n\nnote: Learnable Initial Conditions\nBreadth and Depth of Neural Network\n\nnote: Variable Transformation to $u=1/r$\nOne idea I am considering is a variable transformation from r to u equiv 1r. In this new set of variables, consider the angular component of the metric. The newtonian angular component looks as follows under this variable transformation. In terms of r, the angular component was originally g_N^phiphi(r)=r^-2 (1+frac2r)^-1. In terms of u, this becomes g_N^phiphi(u)=u^2 * (1+2*u)^-1. Thus, if written in the form g_S^phiphi=r^-2, the goal of the NN under this variable transformation would be to learn (1+2*u). Alternatively, if formulated in the form g^phiphi_S=g^phiphi_N * (1+f^phiphi_NN), then the NN must learn only f^phiphi_NN = 2*u. ","category":"section"},{"location":"guide/dissipative_orbits/#Simulating-Orbits-with-GENERIC","page":"Simulating Orbits with GENERIC","title":"Simulating Orbits with GENERIC","text":"How to install, load data, and run a tiny example…","category":"section"},{"location":"guide/orbits/#Learning-the-Schwarzschild-Metric","page":"Simulating Orbits in Schwarzschild Metric","title":"Learning the Schwarzschild Metric","text":"We will now discuss how our neural network seeks to learn the Schwarzschild Metric. The setup is as follows: The NN begins with the Newtonian weak-field limit metric as an ansatz and attempts to learn the Schwarzschild Metric by fitting the gravitational waveform. To demonstrate the scale of the problem, the (inverse) Newtonian Metric is as follows. We use the inverse metric (as opposed to the standard metric) by virtue of the way that we construct the Hamiltonian: \n\ng^munu_N = beginpmatrix -left(1-frac2r right)^-1  0  0  0  \n                        0  left(1+frac2rright)^-1  0  0  \n                        0  0  0  0 \n                        0  0  0  r^-2 * left(1 + frac2r right)^-1\nendpmatrix\n\nThe goal is to learn the (inverse) Schwarzschild metric: \n\ng^munu_S = beginpmatrix -left(1-frac2r right)^-1  0  0  0  \n                        0  left(1-frac2rright)^+1  0  0  \n                        0  0  0  0 \n                        0  0  0  r^-2\nendpmatrix\n\nNow that we've set the stage, we see that the temporal component of both metrics g^tt are the same. The neural network is tasked only with learning g^rr and g^phiphi. How hard exactly is this problem? We can plot the radial and angular components for both metrics as a function of radius to see exactly what the NN has to learn: \n\n(Image: RadialComponent)\n\n(Image: AngularComponent)\n\nWe will now construct our metric as follows, with the intention of the neural network learning 4 quantities:\n\nInitial Conditions: The Neural Network will learn additive corrections to the initial conditions (EL) of the binary black holes\nMetric Components: The Neural Network will learn multiplicative corrections to the metric components g^tt and g^rr of the Newtonian weak-field limit metric (and thereby attempt to learn the Schwarzschild metric)\n\nWe will now provide a full, step-by-step breakdown of our code for how we will achieve the above aims, with the ultimate result being that we will generate the metric fits for the following two orbits: (p=15 e=05) and (p=15 e=09). Here are the steps we will take: \n\nStep 0: Generate Training Data\nStep 1: Import Training Data\nStep 2: Define Simulation Parameters\nStep 3: Define Initial Conditions\nStep 4: Create Neural Networks\nStep 5: Initialize Neural Networks\nStep 6: Assign NN Inputs & Extract Outputs\nStep 6A: Create Helper Function to Construct Initial Conditions\nStep 7: Create Function for ODE Model\nStep 8: Define & Solve ODE Model & Convert Orbit to Waveform\nStep 8.1: Test Initial Solution\nStep 9: Define Loss Function\nStep 10: Run BFGS Optimization Algorithm\n\nI will take you through this entire pipeline step-by-step. First, we begin with Step 0: Generating the training data. We will do this for both (p=15 e=05) and (p=15 e=09).\n\nnote: Step 0: Generate Training Data\nIn our BareBonesSchwarzschild.jl code, we set the following parameters: p = 15, e = 0.5. To rapidly iterate and test our code, I will use a very small datasize=100, which calls for a correspondingly small timespan tspan = (0, 2e3). Otherwise, if we choose a longer timespan, the orbit will be very choppy, because 100 data points will be spread thin and not sample the data as densely.  (Image: BlackHolesp15e05) (Image: GWp1505)\n\nnote: Step 1: Import Training Data\nIn our FullNewtonian2Schwarzschild.jl code, we import our true orbits and waveforms as simply as:  x_ecc, y_ecc = file2trajectory(tsteps,\"input/trajectoryA_Schwarzschild_p15_e0p5.txt\")\n  x2_ecc, y2_ecc = file2trajectory(tsteps,\"input/trajectoryB_Schwarzschild_p15_e0p5.txt\")\n  waveform_real_ecc = file2waveform(tsteps,\"input/waveform_real_Schwarzschild_p15_e0p5.txt\")\n  waveform_imag_ecc = file2waveform(tsteps,\"input/waveform_imag_Schwarzschild_p15_e0p5.txt\")\n\nnote: Step 2: Define Simulation Parameters\nWe define our simulation parameters simply as follows. The masses are given by     mass_ratio = 1\n    model_params = [mass_ratio]\n    mass1 = 1.0/(1.0+mass_ratio)\n    mass2 = mass_ratio/(1.0+mass_ratio)We can also define the timesteps and timespan as follows  tspan = (0, 1.9e3)\n  datasize = 100\n  tsteps = range(tspan[1], tspan[2], length = datasize) \n  dt_data = tsteps[2] - tsteps[1]\n  dt = 1.0\n  num_optimization_increments = 20\n\nnote: Step 3: Define Initial Conditions\nWe define our initial conditions as Newtonian initial conditions, as follows:  p = 15     e = 0.5     r_min = p / (1+e)     r_max = p / (1-e)     const rvals_penalty = range(r_min, r_max; length = 100)     E0_base, L0_base = eccentric_pt_L(p, e) # Newtonian IC\n\nnote: Step 4: Create Neural Networks\nOur neural network is relatively simple, with just a single hidden layer:     NN_Conservative = Chain(\n      Dense(1, 10, tanh),\n      Dense(10, 10, tanh),\n      Dense(10, 2),\n    )\n\nnote: Step 5: Initialize Neuarl Networks\nWe define the NN parameters and state as follows: NN_Conservative_params, NN_Conservative_state = Lux.setup(rng, NN_Conservative). Now, we initialize the weights and biases of the NN near zero (near Newtonian weak-field limit conditions):    for (i, layer) in enumerate(NN_Conservative_params)\n        if ~isempty(layer)\n            if i == length(NN_Conservative_params)  # Final layer\n                layer.weight .= 0\n                layer.bias .= 0 # Force output near 0\n            else  # Hidden layers\n                layer.weight .= 0.1 * randn(rng, eltype(layer.weight), size(layer.weight))\n                layer.bias .= 0.1 * randn(rng, eltype(layer.bias), size(layer.bias))\n            end\n        end\n    end\n\nnote: Step 6: Assign NN Inputs & Extract Outputs\nThe only input to our neural network is the radial variable r (In our state vector, that's u[2] because u = [t, r, \\theta, \\phi, p_t, p_r, p_\\theta, p_\\phi]. Thus, we create a function that's able to feed r as the only input to the NN and extract the NN output: function NN_adapter(u, params)\n  conservative_features = [u[2]]\n  conservative_output, _ = NN_Conservative(conservative_features, params.conservative, NN_Conservative_state)    \n  return (conservative = conservative_output)\nendThis adapter function will be used later to define our ODE model via du = GENERIC(du, u, model_params, t, NN=NN_adapter_dual, NN_params=p). We also define a function NN_params to store all the corrections our NN will be making (i.e., store all the NN parameters): NN_params = ComponentArray(\n  conservative = NN_Conservative_params,\n  dE0 = 0.0, \n  dL0 = 0.0\n)\n\nnote: Step 6A: Create Helper Function to Construct Initial Conditions\nNow we create a helper function which will add the NN corrections to the newtonian initial conditions and then construct the new state vector u0 after these corrections:   function make_u0(params)\n    E0 = E0_base + params.dE0\n    L0 = L0_base + params.dL0\n    return [\n        0.0, # t\n        r0, # r\n        pi/2, # θ\n        0.0, # ϕ\n        E0, # pₜ\n        0.0, # pᵣ\n        0.0, # p_θ\n        -L0, # p_ϕ\n        0.0\n    ]\n  end\n\nnote: Step 7: Create Function for ODE Model\nWe define our ODE model quite simply, as follows: function ODE_model(du, u, p, t)\n  du = GENERIC(du, u, model_params, t,\n                                  NN=NN_adapter_dual, \n                                  NN_params=p)\n  return du\nend\n\nnote: Step 8: Define & Solve ODE Model & Convert Orbit to Waveform\nWe now solve the initial ODE problem, obtain the equations of motion, integrate them to obtain the orbits (r(t) phi(t)), and then convert these orbits into the gravitational wave h(t). u0_init = make_u0(NN_params)\nprob_nn = ODEProblem(ODE_model, u0_init, tspan, NN_params)\nsoln_nn = Array(solve(prob_nn_dual, Tsit5(), saveat = tsteps, dt = dt, adaptive=false, verbose=false))\nwaveform_nn_real, waveform_nn_imag = compute_waveform(dt_data, soln_nn, mass_ratio; coorbital=false)\norbit = soln2orbit(soln_nn)\npred_orbit1_init, pred_orbit2_init = one2two(orbit, 1, mass_ratio)And now we plot our initial solution:plot(x_ecc, y_ecc, aspect_ratio=:equal, linewidth = 2, label = \"Real\")\nplot!(pred_orbit1_init[1,:], pred_orbit1_init[2,:], aspect_ratio=:equal, linewidth = 2, label = \"Prediction\")\nplot(waveform_real_ecc, label = \"Real\")\nplot!(waveform_nn_real, label = \"Prediction\")\nplot(waveform_imag_ecc, label = \"Real\")\nplot!(waveform_nn_imag, label = \"Prediction\")\n\nIf we now plot our results, they look as follows. \n\n(Image: TrainingResultsp15)","category":"section"},{"location":"#Binary-Black-Holes:-From-Gravity-Waves-to-Orbits","page":"Home","title":"Binary Black Holes: From Gravity Waves to Orbits","text":"","category":"section"},{"location":"#By-Ref","page":"Home","title":"By Ref","text":"Binary Black Holes are a fascinating physical system, unifying the small (amplitudes on the order of sim 10^-21) and the grand (lumonisities 10^47 ergs, brighter than all the stars in the universe!). For a binary black hole system, the dominant h^22 mode is given by \n\nh^22(t) sim frac1r(ddotI_xx - ddotI_yy - 2iddotI_xy)\n\nThe above terms are given by \n\nI_xx= 2(x^2 - frac13(x^2+y^2))\n\nI_yy= 2(y^2 - frac13(x^2+y^2))\n\nI_xy= xy\n\nFormulating this in Julia is simple: \n\nfunction orbit2tensor(orbit, component, mass=1.0)\n    x, y = orbit[1,:], orbit[2,:]\n\n    Ixx, Iyy, Ixy = x^2, y^2, x*y\n    trace = Ixx + Iyy\n\n    if I[1,1]:\n        I = Ixx .- (1.0 ./ 3.0).*trace\n    elseif I[2,2]:\n        I = Iyy .- (1.0 ./ 3.0).*trace\n    else\n        I = Ixy\n    end\n\n    return mass .* I\nend","category":"section"}]
}
