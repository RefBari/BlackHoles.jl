<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Attempts to Optimize Neural Network Fits · BlackHoles.jl</title><meta name="title" content="Attempts to Optimize Neural Network Fits · BlackHoles.jl"/><meta property="og:title" content="Attempts to Optimize Neural Network Fits · BlackHoles.jl"/><meta property="twitter:title" content="Attempts to Optimize Neural Network Fits · BlackHoles.jl"/><meta name="description" content="Documentation for BlackHoles.jl."/><meta property="og:description" content="Documentation for BlackHoles.jl."/><meta property="twitter:description" content="Documentation for BlackHoles.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">BlackHoles.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Conservative Dynamics</span><ul><li><a class="tocitem" href="../newtonian-metric/">Physics of Newtonian Metric</a></li><li><a class="tocitem" href="../schwarzschild-metric/">Physics of Schwarzschild Metric</a></li><li><a class="tocitem" href="../orbits/">Simulating Orbits in Schwarzschild Metric</a></li><li><a class="tocitem" href="../waveforms/">Quadrupole Approximation: Orbits to Gravitational Wave</a></li><li><a class="tocitem" href="../higher_modes/">Adding Higher Order Modes</a></li><li><a class="tocitem" href="../inverse_problem/">Fundamentals of Neural Networks</a></li><li><a class="tocitem" href="../sanity_checks/">Sanity Checks</a></li></ul></li><li><span class="tocitem">Dissipative Dynamics</span><ul><li><a class="tocitem" href="../GENERIC/">Theory: GENERIC Formalism</a></li><li><a class="tocitem" href="../dissipative_orbits/">Simulating Orbits with GENERIC</a></li><li><a class="tocitem" href="../full_inverse_problem/">Dissipative Case: Waves to Orbits</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Attempts to Optimize Neural Network Fits</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Attempts to Optimize Neural Network Fits</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/RefBari/BlackHoles.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/RefBari/BlackHoles.jl/blob/main/docs/src/guide/optimize.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Attempts-to-Optimize-Neural-Network-Fits"><a class="docs-heading-anchor" href="#Attempts-to-Optimize-Neural-Network-Fits">Attempts to Optimize Neural Network Fits</a><a id="Attempts-to-Optimize-Neural-Network-Fits-1"></a><a class="docs-heading-anchor-permalink" href="#Attempts-to-Optimize-Neural-Network-Fits" title="Permalink"></a></h1><p>In this section, I recount five experiments I ran on the weekend of 11/08 and week of 11/10 to optimize the neural network fits: (1) Tikhonov Regularization, (2) Testing Different Parameters in the <code>Optim.jl</code> library, (3) Variable Transformation to <span>$u=1/r$</span>, (4) Neural Network Size, and (5) Learnable Initial Conditions. </p><div class="admonition is-info" id="Tikhonov-Regularization-679547a25a8a4ae2"><header class="admonition-header">Tikhonov Regularization<a class="admonition-anchor" href="#Tikhonov-Regularization-679547a25a8a4ae2" title="Permalink"></a></header><div class="admonition-body"><p>Testing test test</p></div></div><div class="admonition is-info" id="Optim.jl-Parameters-6368e3572ee5c634"><header class="admonition-header">`Optim.jl` Parameters<a class="admonition-anchor" href="#Optim.jl-Parameters-6368e3572ee5c634" title="Permalink"></a></header><div class="admonition-body"><p>The heart and soul of the optimizer is the BFGS Algorithm. We now provide a brief intro to the BFGS algorithm. The idea is simple: we have some function <span>$f(x)$</span> which we seek to minimize. To understand BFGS, we turn to its predecessor, Newton&#39;s method. Newton&#39;s method forms a second order approximation of <span>$f(x)$</span> at a given point and finds the minimum of this approximation. Specifically, say we&#39;d like to minimize <span>$f(x)$</span> starting at <span>$x_0$</span>. If we take a linear Taylor expansion of <span>$f&#39;(x)$</span>, we have <span>$f&#39;(x) \approx f&#39;(x_0)+f&#39;&#39;(x_0)(x-x_0)$</span>. If we seek to minimize <span>$f(x)$</span>, then we seek <span>$f&#39;(x) = 0$</span>. In that case, <span>$f&#39;(x_0) + f&#39;&#39;(x_0)x-f&#39;&#39;(x_0)x_0 = 0$</span>. If we now solve for <span>$x$</span>, we find that <span>$x^* = x_0 - \frac{f&#39;(x_0)}{f&#39;&#39;(x_0)}$</span>. This method of finding the minimum of a function is known as Newton&#39;s method. Newton&#39;s method requires both the gradient <span>$\nabla f = f&#39;(x)$</span> and the Hessian <span>$\nabla^2 f = f&#39;&#39;(x)$</span>. We can write Newton&#39;s method as <span>$x^* = x_0 - [\nabla^2 f(x_0)]^{-1} \nabla f(x_0)$</span>. But this can be difficult for several reasons: finding the inverse Hessian could be difficult. What if there was a way we could approximate the Hessian (or its inverse)? We now consider the BFGS algorithm. Consider again our goal of minimizing the function <span>$f(x)$</span>. We are iteratively updating the step <span>$x_k$</span> where <span>$k=1, 2, ..., T$</span>, where <span>$T$</span> is determined by some convergence criterion. Suppose we use a quadratic approximation to <span>$f$</span> at each iteration. Denote this approximation at step <span>$k$</span> as <span>$\hat{f}_k(x)$</span>. Specifically, <span>$\hat{f}_k=f(x_k) + [\nabla f(x_k)]^T (x-x_k) + \frac{1}{2}(x-x_k)^T [\nabla f(x_k)]^2 (x-x_k)$</span>.</p></div></div><div class="admonition is-info" id="Neural-Network-Size-4c33d387c8a71438"><header class="admonition-header">Neural Network Size<a class="admonition-anchor" href="#Neural-Network-Size-4c33d387c8a71438" title="Permalink"></a></header><div class="admonition-body"><p>Breadth and Depth of Neural Network</p></div></div><div class="admonition is-info" id="Learnable-Initial-Conditions-e53bef2b3562bbc"><header class="admonition-header">Learnable Initial Conditions<a class="admonition-anchor" href="#Learnable-Initial-Conditions-e53bef2b3562bbc" title="Permalink"></a></header><div class="admonition-body"><p>Breadth and Depth of Neural Network</p></div></div><div class="admonition is-info" id="Variable-Transformation-to-u1/r-dd800549402d8029"><header class="admonition-header">Variable Transformation to $u=1/r$<a class="admonition-anchor" href="#Variable-Transformation-to-u1/r-dd800549402d8029" title="Permalink"></a></header><div class="admonition-body"><p>One idea I am considering is a variable transformation from <span>$r$</span> to <span>$u \equiv 1/r$</span>. In this new set of variables, consider the angular component of the metric. The newtonian angular component looks as follows under this variable transformation. In terms of <span>$r$</span>, the angular component was originally <span>$g_{N}^{\phi\phi}(r)=r^{-2} (1+\frac{2}{r})^{-1}$</span>. In terms of <span>$u$</span>, this becomes <span>$g_{N}^{\phi\phi}(u)=u^2 * (1+2*u)^{-1}$</span>. Thus, if written in the form <span>$g_{S}^{\phi\phi}=r^{-2}$</span>, the goal of the NN under this variable transformation would be to learn <span>$(1+2*u)$</span>. Alternatively, if formulated in the form <span>$g^{\phi\phi}_{S}=g^{\phi\phi}_{N} * (1+f^{\phi\phi}_{NN})$</span>, then the NN must learn only <span>$f^{\phi\phi}_{NN} = 2*u$</span>. </p></div></div></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.0 on <span class="colophon-date" title="Monday 17 November 2025 06:21">Monday 17 November 2025</span>. Using Julia version 1.12.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
