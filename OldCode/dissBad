using LinearAlgebra
using Plots
using DifferentialEquations
using LaTeXStrings
using Optimization, OptimizationOptimJL
using Optimisers
using OptimizationOptimisers
using Printf
using Plots.Measures
using Lux
using SciMLSensitivity
using ComponentArrays
using Random
using ForwardDiff
using ProgressMeter
using CSV
using DelimitedFiles
using LaTeXStrings

plot_font = "Computer Modern"
default(
    fontfamily = plot_font,
    linewidth = 2,
    framestyle = :box,
    label = nothing,
    grid = false,
)
Plots.scalefontsizes()

"""
Store all data in these globally defined arrays. 
"""

losses = []
partition_boundaries = []
final_paramaters = []
solutions_list = []
parameters_list = []
final_predicted_h_plus = []
training_h_plus_wave = []

"""
Goal: Define the Optimize Black Hole Function
Purpose: Optimize a Neural Network to learn the Schwarzschild Metric 
         from the Newtonian Weak-Field Limit Metric by fitting a gravitational 
         wave generated by a binary black hole system. The inputs are 
         the initial conditions (p,e) which determine the initial 
         state vector of the particle orbiting the Schwarzschild
         black hole. 
"""

function optimizeBlackHole(;
    learningRate,
    epochsPerIteration,
    numberOfCycles,
    totalTrainingPercent,
    true_parameters,
    initial_guess,
)
    # Restart global variables every time we call function
    global losses = []
    global partition_boundaries = []
    global final_paramaters = []
    global solutions_list = []
    global parameters_list = []
    global final_predicted_h_plus = []
    global training_h_plus_wave = []


    trainingFraction = totalTrainingPercent # What total fraction of the training data will the neural network learn from?

    """
        Step 1: Obtain True Gravitational Wave Data
    """
    gw_data = readdlm("/Users/rbari/Work/BlackHoles/scripts/gw_data.csv", ',')

    times_real = gw_data[2:end, 1] # 2:end b/c first row is header
    h_real_data = gw_data[2:end, 2]

    t0 = times_real[1]
    t_data = times_real .- t0
    T_data = t_data[end]

    p_guess = pe_2_EL(initial_guess[1], initial_guess[2])
    # timestep = 10 # Timestep for ODE Solver and Optimizer

    """
        Step 2: Define the Neural ODE in GENERIC Form
    """
    function SchwarzschildHamiltonian_GENERIC(du, u, p, t)
        x = @view u[1:8] # u[1] = t, u[2] = r, u[3] = θ, u[4] = ϕ  
        NN_params = p.NN

        M, E, L = p.parameters.M, p.parameters.E, p.parameters.L

        NN_correction = NN([x[2]], NN_params, NN_state)[1]

        function H(state_vec)
            t, r, θ, φ, p_t, p_r, p_θ, p_ϕ = state_vec

            f = (1 - (2/r))
            # print("\n\nTrue g_tt:", -f^(-1))
            p = [p_t, p_r, p_θ, p_ϕ]
            g = [
                -f^(-1) 0 0 0;
                0 f 0 0;
                0 0 0 0;
                0 0 0 r^(-2)
            ]

            H_schwarzschild = (1/2) * p' * g * p

            return H_schwarzschild # Returns equations of motion in PROPER time
        end

        # Compute gradient using ForwardDiff
        grad_H = ForwardDiff.gradient(H, x)

        # Define symplectic matrix L (8x8)
        J = [
            zeros(4, 4) I(4);
            -I(4) zeros(4, 4)
        ]

        M15 = 0
        M35 = 0
        M45 = NN_correction[1] # p_ϕ
        M25 = 0
        
        Conservative = J * grad_H
        c4 = Conservative[4] # ϕ̇ = ∂H/∂p_ϕ
        M55 = M45 * c4

        # Dissipation corrections to ...
        Dissipation = [
            0, # t
            M15, # r
            0, # θ
            M25, # ϕ
            0, # p_t
            M35, # p_r
            0,  # p_θ
            M45] # p_ϕ

        # Hamilton's equations: ẋ = J*∇H
        du_dτ = Conservative + Dissipation

        dH_dpₜ = grad_H[5]
        dτ_dt = (dH_dpₜ)^(-1)

        @inbounds for i = 1:8
            du[i] = du_dτ[i] * dτ_dt
        end
        du[9] = M55
        du[1] = 1
    end

    # Neural network setup 
    NN = Chain(
        Dense(1, 4, tanh), # Learns correction term in terms of 4 parameters: r, p_t, p_r, p_ϕ
        Dense(4, 4, tanh),
        Dense(4, 1),
    )        # Output 3 corrections, g^tt, g^rr, g^ϕϕ

    rng = MersenneTwister(222)
    NN_params, NN_state = Lux.setup(rng, NN)
    precision = Float64
    NN_params = Lux.fmap(x -> precision.(x), NN_params)

    # Same weight initialization as your original
    for layer in NN_params
        if ~isempty(layer)
            layer.weight .= 0.1 * randn(rng, eltype(layer.weight), size(layer.weight))
            layer.bias .= 0.02 * randn(rng, eltype(layer.bias), size(layer.bias))
        end
    end

    # What will the neural network learn? The following parameters: M, E, L, NN_param
    R = initial_guess[1]/(1-initial_guess[2])
    M = p_guess[1]
    E = p_guess[2]
    L = p_guess[3]

    θ = (; NN = NN_params, parameters = (M = M, E = E, L = L))
    θ = ComponentVector{precision}(θ);

    """
        Step 3: Define Initial Conditions for State Vector of Particle Orbiting Schwarzschild BH
    """
    # Initial State Vector for Particle orbiting Schwarzschild BH
    # u0 = [0, R, pi/2, 0, -1*E, 0, 0, L] # u = [t₀, r₀, θ₀, ϕ₀, pₜ₀, pᵣ₀, p_θ₀, p_ϕ₀]
    # Use relativistic constraint H = -1/2
    g_tt_factor = 1 - 2/R
    g_rr_factor = 1 + 2/R
    angular_term = (L^2 / R^2) * (1/g_rr_factor)
    p_t_squared = g_tt_factor * (1 + angular_term)  # For H = -1/2
    p_t_correct = -sqrt(p_t_squared)

    u0 = [0, R, π/2, 0, p_t_correct, 0, 0, L, 0]

    # Define the ODE Problem using the Neural Network
    prob_learn = ODEProblem(SchwarzschildHamiltonian_GENERIC, u0, (0, T_data), θ)

    """
        Step 4: Define Loss Function
    """
    function loss(pn, trainingFraction)
        target_time = trainingFraction * t_data[end]
        n = searchsortedfirst(t_data, target_time)
        
        t_win = @view t_data[1:n]
        y_win = @view h_real_data[1:n]
    
        prob  = remake(prob_learn; p = pn, tspan = (0.0, t_win[end]))
        sol   = solve(prob, Tsit5(); saveat = t_win, adaptive=true, reltol=1e-8, abstol=1e-12)
    
        ĥ     = compute_waveform(mean(diff(t_win)), sol, 1.0; coorbital=false)[1]
        m = min(length(ĥ), length(y_win))
        return mean(abs2, @view(ĥ[1:m]) .- @view(y_win[1:m]))
    end
    
    """
        Step 5: Define Callback Function
    """
    function callback(pn, loss; dotrain = true)
        if dotrain
            push!(losses, loss);
            @printf("Epoch: %d, Loss: %15.12f \n", length(losses), loss);
            p = plot(
                losses,
                label = "Loss",
                xlabel = "Epochs",
                ylabel = "Loss",
                yaxis = :log,
                top_margin = 10mm,
                bottom_margin = 10mm,
                left_margin = 10mm,
                right_margin = 10mm,
            )
            vline!(partition_boundaries, label = "Partition")
            display(p)
        end
        return false
    end

    """
        Step 6: Define BFGS Optimizer
    """
    # define Optimization Problem
    adtype = Optimization.AutoZygote() # instead of Optimization.AutoZygote(), use finite differences
    optf = Optimization.OptimizationFunction(
        (x, p) -> loss(x, trainingFraction/numberOfCycles),
        adtype,
    )
    θ_init = θ;

    # create Optimization Problem (function + initial guess for parameters)
    optprob = Optimization.OptimizationProblem(optf, θ_init)

    # choose method for solving problem 
    lr = learningRate;

    # solve the problem
    num_iters = epochsPerIteration; # number of iterations per partition (i.e., 2 partitions means one run + 2 additional runs = 3 runs * 25 epochs/run = 75 epochs)
    opt_result = Optimization.solve(
        optprob,
        Optim.BFGS(; initial_stepnorm = lr),
        callback = callback,
        maxiters = num_iters,
    )
    θ_final = opt_result.u

    NN_params_final = θ_final.NN

    newprob = remake(prob_learn, p = θ_final)
    sol = solve(newprob, Tsit5(), saveat = t_data)
    push!(solutions_list, sol)

    h_plus_pred = compute_waveform(mean(diff(t_data)), sol, 1.0; coorbital = false)[1]
    h_plus_training = h_real_data

    n_pred = length(h_plus_pred)
    n_train = length(h_plus_training)

    if n_pred == n_train
        # Same length - no padding needed
        h_plus_pred_plot = h_plus_pred
    elseif n_pred < n_train
        # Predicted is shorter - pad with zeros
        h_plus_pred_plot = [h_plus_pred; zeros(n_train - n_pred)]
    else  # n_pred > n_train
        # Predicted is longer - truncate
        h_plus_pred_plot = h_plus_pred[1:n_train]
    end

    t_plot = t_data

    p = plot(
        t_plot,
        h_plus_training,
        label = L"$h_+$ true",
        xlabel = "Time (s)",
        ylabel = L"h_+",
        legend = :topright,
        color = "lightsalmon",
        linewidth = 2,
        grid = true,
        top_margin = 10mm,
        bottom_margin = 10mm,
        left_margin = 10mm,
        right_margin = 10mm,
    )
    plot!(t_plot, h_plus_pred_plot, label = L"$h_+$ predicted", color = "blue")# plot!(t_plot, predicted_waveform_plus_old_padded, label="h+ initial prediction", linewidth=2)
    
    # trainingWindowTime = t_data[n_train]
    target_time_0 = (totalTrainingPercent/numberOfCycles) * t_data[end]
    n0 = searchsortedfirst(t_data, target_time_0)
    trainingWindowTime = t_data[n0]

    vline!(
        [trainingWindowTime],
        color = "red",
        linestyle = :dash,
        label = "Training Window",
    )
    display(p)

    """
        Step 7: Train Optimizer over Iteratively Longer Sections of Training Data 
    """
    function partitionTraining(numCycles, totalTrainingFraction)
        global partition_boundaries, losses, final_paramaters, solutions_list, parameters_list

        amountTrain = totalTrainingFraction / numCycles
        p_final_array = [θ_final]

        @showprogress for i = 2:numCycles
            frac_i  = i * amountTrain
            target_time = frac_i * t_data[end]
            n_train = searchsortedfirst(t_data, target_time)

            t_win   = @view t_data[1:n_train]
            y_win   = @view h_real_data[1:n_train]
            
            trainingFraction = i * amountTrain
            push!(partition_boundaries, length(losses))
            push!(final_paramaters, p_final_array[end])
            optf = Optimization.OptimizationFunction(
                (x, p) -> loss(x, trainingFraction),
                adtype,
            )

            θ_current = p_final_array[end]
            optprob = Optimization.OptimizationProblem(optf, θ_current)
            opt_result_2 = Optimization.solve(
                optprob,
                Optim.BFGS(; initial_stepnorm = lr),
                callback = callback,
                maxiters = num_iters,
            )

            θ_final_2 = opt_result_2.u;
            push!(p_final_array, θ_final_2)
            newprob_2 = remake(prob_learn, p = θ_final_2)
            sol_2 = solve(newprob_2, Tsit5(), saveat = t_win)
            push!(solutions_list, sol_2)
            push!(parameters_list, getParameters(sol_2))

            # Extract predicted and true solution components
            tarr = sol_2.t
            y2 = sol_2[2, :] # r (radial) coordinate solutions  
            y4 = sol_2[4, :] # planar angle solutions
            y9 = sol_2[9, :] # M55

            dissPlot = plot(tarr, y9, xlabel="t", ylabel="S", label="S",
            linewidth=2, top_margin=10mm, bottom_margin=10mm, 
            left_margin=10mm, right_margin=10mm, title = "\nDissipation")
            display(dissPlot)
            savefig("Dissipation.png")
            
            # Create orbital trajectory plot
            particle_x = y2 .* cos.(y4)
            particle_y = y2 .* sin.(y4)

            # r_plot = plot(t, y2, label = L"$r$ predicted", title = "\n\nRadial Coordinate")

            p2 = plot(
                particle_x,
                particle_y,
                aspect_ratio = 1,
                linewidth = 2,
                title = "\nFinal Predicted Trajectory",
                xlabel = L"x",
                ylabel = L"y",
                label = "Predicted Trajectory",
                bottom_margin = 5mm,
                top_margin = 5mm,
                left_margin = 5mm,
                right_margin = 5mm,
            )

            display(p2)

            h_plus_pred = compute_waveform(mean(diff(t_data)), sol_2, 1.0; coorbital = false)[1]

            # Handle all cases properly
            n_pred = length(h_plus_pred)
            n_full = length(h_plus_training)

            if n_pred == n_full
                # Same length - no padding needed
                h_plus_pred_plot = h_plus_pred
            elseif n_pred < n_full
                # Predicted is shorter - pad with zeros
                h_plus_pred_plot = [h_plus_pred; zeros(n_full - n_pred)]
            else  # ADD THIS: when n_pred > n_train
                h_plus_pred_plot = h_plus_pred[1:n_full]
            end

            t_plot = t_data
            println("Final iteration check:")
            println("Should be training on: $(totalTrainingPercent * 100)%")
            println("n_train: $n_train")  
            println("Corresponds to time: $(t_data[n_train])")
            println("Total time span: $(t_data[end])")
            println("Actual percentage: $(100 * t_data[n_train] / t_data[end])%")
            
            p = plot(
                t_plot,
                h_plus_training,
                color = "lightsalmon",
                label = L"$h_+$ true",
                linewidth = 2,
                xlabel = "Time (s)",
                ylabel = L"h_+",
                legend = :topright,
                grid = true,
                bottom_margin = 10mm,
                top_margin = 10mm,
                left_margin = 10mm,
                right_margin = 10mm,
            )
            plot!(t_plot, h_plus_pred_plot, label = L"$h_+$ predicted", color = "blue")# plot!(t_plot, predicted_waveform_plus_old_padded, label="h+ initial prediction", linewidth=2)
            trainingWindowTime = t_data[n_train]
            vline!(
                [trainingWindowTime],
                color = "red",
                linestyle = :dash,
                label = "Training Window",
            )
            display(p)

            push!(final_predicted_h_plus, h_plus_pred_plot)
            push!(training_h_plus_wave, h_plus_training)
        end
    end

    numCycles = numberOfCycles

    partitionTraining(numCycles, trainingFraction)

    return losses[end-1]
end

optimizeBlackHole(
    learningRate = 1e-3,
    epochsPerIteration = 2,
    numberOfCycles = 3,
    totalTrainingPercent = 0.80,
    true_parameters = [13, 0.0001529],
    initial_guess = [13, 0.0001529],
)
